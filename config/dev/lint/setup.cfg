# Use NVIDIA's Triton Inference Server as base
FROM nvcr.io/nvidia/tritonserver:23.12-py3

# Set working directory
WORKDIR /app

# Install system dependencies
RUN apt-get update && apt-get install -y \
    curl \
    git \
    python3-pip \
    python3-dev \
    build-essential \
    && rm -rf /var/lib/apt/lists/*

# Install poetry
RUN curl -sSL https://install.python-poetry.org | python3 - \
    && poetry config virtualenvs.create false

# Set environment variables
ENV NVIDIA_VISIBLE_DEVICES=all \
    NVIDIA_DRIVER_CAPABILITIES=compute,utility \
    PYTHONUNBUFFERED=1 \
    MODEL_REPOSITORY=/models \
    PYTHONPATH="${PYTHONPATH}:/app"

# Copy dependency files
COPY pyproject.toml poetry.lock requirements.txt ./

# Install base dependencies first
RUN pip install --upgrade pip && \
    pip install wheel setuptools && \
    pip install nvidia-pyindex

# Install NVIDIA dependencies
RUN pip install nvidia-cuda-runtime-cu12 nvidia-cublas-cu12 nvidia-cudnn-cu12 && \
    pip install tritonclient[all]

# Install Python dependencies
RUN pip install -r requirements.txt

# Install poetry dependencies
RUN poetry install --no-dev --no-root

# Copy the rest of the application
COPY srt_model_quantizing ./srt_model_quantizing
COPY README.md ./

# Install the application
RUN poetry install --no-dev

# Create directories for models and ensure proper permissions
RUN mkdir -p /models /quantized_models && \
    chmod -R 755 /models /quantized_models

# Create a non-root user
RUN useradd -m -u 1000 triton && \
    chown -R triton:triton /app /models /quantized_models

# Switch to non-root user
USER triton

# Create volume mount points
VOLUME ["/models", "/quantized_models"]

# Expose ports
EXPOSE 8000 8001 8002

# Health check
HEALTHCHECK --interval=30s --timeout=30s --start-period=5s --retries=3 \
    CMD curl -f http://localhost:8000/v2/health/ready || exit 1

# Set entrypoint
ENTRYPOINT ["tritonserver"]
CMD ["--model-repository=/models", "--http-port=8000", "--grpc-port=8001", "--metrics-port=8002"] 